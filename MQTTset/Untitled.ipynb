{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8487616-9580-451a-9ab8-09cab584fee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install scikeras\n",
    "# !pip install TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbabcd7b-83a6-432d-82a5-120f0bb6b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 04:15:18.421930: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-30 04:15:18.572053: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-30 04:15:18.572174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-30 04:15:18.575381: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-30 04:15:18.594578: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-30 04:15:18.595914: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-30 04:15:21.026996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to generate train and test datasets\n",
      "Starting Random forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 04:15:47.279878: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "232/232 - 3s - loss: 0.9414 - accuracy: 0.7656 - val_loss: 0.6330 - val_accuracy: 0.7799 - 3s/epoch - 14ms/step\n",
      "Epoch 2/200\n",
      "232/232 - 1s - loss: 0.5293 - accuracy: 0.7868 - val_loss: 0.5115 - val_accuracy: 0.7894 - 1s/epoch - 6ms/step\n",
      "Epoch 3/200\n",
      "232/232 - 1s - loss: 0.4509 - accuracy: 0.8115 - val_loss: 0.4705 - val_accuracy: 0.8222 - 1s/epoch - 6ms/step\n",
      "Epoch 4/200\n",
      "232/232 - 1s - loss: 0.4238 - accuracy: 0.8540 - val_loss: 0.4275 - val_accuracy: 0.8815 - 1s/epoch - 6ms/step\n",
      "Epoch 5/200\n",
      "232/232 - 1s - loss: 0.3663 - accuracy: 0.8844 - val_loss: 0.3956 - val_accuracy: 0.8572 - 1s/epoch - 6ms/step\n",
      "Epoch 6/200\n",
      "232/232 - 1s - loss: 0.3283 - accuracy: 0.8905 - val_loss: 0.3784 - val_accuracy: 0.8699 - 1s/epoch - 6ms/step\n",
      "Epoch 7/200\n",
      "232/232 - 1s - loss: 0.2974 - accuracy: 0.9020 - val_loss: 0.3598 - val_accuracy: 0.8690 - 1s/epoch - 6ms/step\n",
      "Epoch 8/200\n",
      "232/232 - 1s - loss: 0.2774 - accuracy: 0.9091 - val_loss: 0.3558 - val_accuracy: 0.8746 - 1s/epoch - 6ms/step\n",
      "Epoch 9/200\n",
      "232/232 - 1s - loss: 0.2566 - accuracy: 0.9144 - val_loss: 0.3500 - val_accuracy: 0.8817 - 1s/epoch - 6ms/step\n",
      "Epoch 10/200\n",
      "232/232 - 1s - loss: 0.2463 - accuracy: 0.9142 - val_loss: 0.3535 - val_accuracy: 0.8861 - 1s/epoch - 6ms/step\n",
      "Epoch 11/200\n",
      "232/232 - 1s - loss: 0.2306 - accuracy: 0.9172 - val_loss: 0.3506 - val_accuracy: 0.8827 - 1s/epoch - 6ms/step\n",
      "Epoch 12/200\n",
      "232/232 - 1s - loss: 0.2170 - accuracy: 0.9195 - val_loss: 0.3592 - val_accuracy: 0.8779 - 1s/epoch - 6ms/step\n",
      "Epoch 13/200\n",
      "232/232 - 1s - loss: 0.2222 - accuracy: 0.9176 - val_loss: 0.3272 - val_accuracy: 0.8889 - 1s/epoch - 6ms/step\n",
      "Epoch 14/200\n",
      "232/232 - 1s - loss: 0.2088 - accuracy: 0.9231 - val_loss: 0.3419 - val_accuracy: 0.8837 - 1s/epoch - 6ms/step\n",
      "Epoch 15/200\n",
      "232/232 - 2s - loss: 0.2001 - accuracy: 0.9252 - val_loss: 0.3085 - val_accuracy: 0.9026 - 2s/epoch - 7ms/step\n",
      "Epoch 16/200\n",
      "232/232 - 1s - loss: 0.2010 - accuracy: 0.9252 - val_loss: 0.3153 - val_accuracy: 0.8986 - 1s/epoch - 6ms/step\n",
      "Epoch 17/200\n",
      "232/232 - 2s - loss: 0.1936 - accuracy: 0.9280 - val_loss: 0.3201 - val_accuracy: 0.8960 - 2s/epoch - 7ms/step\n",
      "Epoch 18/200\n",
      "232/232 - 2s - loss: 0.2062 - accuracy: 0.9250 - val_loss: 0.3479 - val_accuracy: 0.8847 - 2s/epoch - 7ms/step\n",
      "Epoch 19/200\n",
      "232/232 - 2s - loss: 0.2085 - accuracy: 0.9242 - val_loss: 0.3921 - val_accuracy: 0.8482 - 2s/epoch - 7ms/step\n",
      "Epoch 20/200\n",
      "232/232 - 1s - loss: 0.1988 - accuracy: 0.9287 - val_loss: 0.3525 - val_accuracy: 0.8894 - 1s/epoch - 6ms/step\n",
      "Epoch 20: early stopping\n",
      "Training time: 55.457953453063965\n",
      "3103/3103 [==============================] - 5s 1ms/step\n",
      "Test time: 7.546294927597046\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                1700      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                1530      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                620       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 126       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3976 (15.53 KB)\n",
      "Trainable params: 3976 (15.53 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Starting Random forest\n",
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   10.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n",
      "Training time: 89.80718731880188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    1.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test time: 2.5741384029388428\n",
      "Starting Naive Bayes\n",
      "Training time: 92.78470349311829\n",
      "Test time: 0.31665635108947754\n",
      "Starting Decision tree\n",
      "Training time: 94.01336932182312\n",
      "Test time: 0.06181454658508301\n",
      "Starting Multi layer perceptron\n",
      "Iteration 1, loss = 2.97328868\n",
      "Iteration 2, loss = 0.63466955\n",
      "Iteration 3, loss = 0.61726814\n",
      "Iteration 4, loss = 0.37697270\n",
      "Iteration 5, loss = 0.71520041\n",
      "Iteration 6, loss = 0.48683119\n",
      "Iteration 7, loss = 0.62728789\n",
      "Iteration 8, loss = 0.41089700\n",
      "Iteration 9, loss = 0.31947532\n",
      "Iteration 10, loss = 0.32963145\n",
      "Iteration 11, loss = 0.59095642\n",
      "Iteration 12, loss = 0.40874493\n",
      "Iteration 13, loss = 0.37125176\n",
      "Iteration 14, loss = 0.43176502\n",
      "Iteration 15, loss = 0.52887703\n",
      "Iteration 16, loss = 0.44351919\n",
      "Iteration 17, loss = 0.34547609\n",
      "Iteration 18, loss = 0.40683189\n",
      "Iteration 19, loss = 0.49532166\n",
      "Iteration 20, loss = 0.34098056\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training time: 559.6124114990234\n",
      "Test time: 0.657710075378418\n",
      "Starting Gradient boost\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8918            1.36m\n",
      "         2           0.7674            1.28m\n",
      "         3           0.6795            1.21m\n",
      "         4           0.6010            1.15m\n",
      "         5           0.5408            1.07m\n",
      "         6           0.4943            1.01m\n",
      "         7           0.4595           56.50s\n",
      "         8           0.4254           52.15s\n",
      "         9           0.3983           47.87s\n",
      "        10           0.3768           43.56s\n",
      "        11           0.3539           39.19s\n",
      "        12           0.3375           34.87s\n",
      "        13           0.3203           30.54s\n",
      "        14           0.3080           26.21s\n",
      "        15           0.2939           21.83s\n",
      "        16           0.2834           17.48s\n",
      "        17           0.2752           13.14s\n",
      "        18           0.2665            8.78s\n",
      "        19           0.2575            4.39s\n",
      "        20           0.2496            0.00s\n",
      "Training time: 648.4966042041779\n",
      "Test time: 0.6262893676757812\n",
      "Decision Tree, accuracy: 0.9031221673884581 F1 score:0.9008991541299932\n",
      "[[ 3312   571     0     7   461     0]\n",
      " [  212 35556     0  3250    59     0]\n",
      " [    1     4    90    88     1     0]\n",
      " [   19  3144     0 46468     8     0]\n",
      " [  992   331    13   456  1486     0]\n",
      " [    0     0     0     0     2  2759]]\n",
      "Naive Bayes, accuracy: 0.670863128210293 F1 score:0.7581608917548786\n",
      "[[ 4290    38     8     5    10     0]\n",
      " [11198 27866     0    13     0     0]\n",
      " [   93     0    89     2     0     0]\n",
      " [18473     0     0 31166     0     0]\n",
      " [ 2676    94    36    34   438     0]\n",
      " [    0     0     0     0     0  2761]]\n",
      "Neural network, accuracy: 0.889404773894652 F1 score:0.887018806899987\n",
      "[[ 2377   499     1    17    91  1366]\n",
      " [  173 34957     0  3815    51    81]\n",
      " [    1    18    74    89     0     2]\n",
      " [    0  2884     0 46703     2    50]\n",
      " [  816   270     1    76  1437   678]\n",
      " [    0     0     0     0     0  2761]]\n",
      "MultiLayerPerceptron, accuracy: 0.8711149159029107 F1 score:0.8722537144053696\n",
      "[[ 2540   682     5   189   935     0]\n",
      " [  216 34932     0  3819   110     0]\n",
      " [    1     3    89    89     2     0]\n",
      " [ 1536  2886     0 45168    49     0]\n",
      " [  736   513    14   394  1619     2]\n",
      " [    6     0     0     0   610  2145]]\n",
      "Random Forest, accuracy: 0.9029308087420687 F1 score:0.9009131314572056\n",
      "[[ 3227   572     0     7   545     0]\n",
      " [  206 35554     0  3250    67     0]\n",
      " [    1     4    89    89     1     0]\n",
      " [   19  3144     0 46468     8     0]\n",
      " [  951   317     0   457  1553     0]\n",
      " [    0     0     0     0     0  2761]]\n",
      "GradienBoost, accuracy: 0.7931513747608017 F1 score:0.8268049294767623\n",
      "[[ 3084   506   497   261     3     0]\n",
      " [  296 25185  9666  3930     0     0]\n",
      " [    2     3     1   178     0     0]\n",
      " [   49  2710     0 46880     0     0]\n",
      " [  991   362   327   642   956     0]\n",
      " [    0     0   115     0     0  2646]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from six import StringIO  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as pyplot\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from warnings import simplefilter\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#with two dataset splitted\n",
    "dftrain = pd.read_csv(\"train70_reduced.csv\") \n",
    "dftest = pd.read_csv(\"test30_reduced.csv\")\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "seed = 7\n",
    "\n",
    "'''\n",
    "#one dataset to be splitted\n",
    "df = pd.read_csv(\"mqttdataset.csv\") \n",
    "seed = 7\n",
    "class_names = df.target.unique()\n",
    "df=df.astype('category')\n",
    "cat_columns = df.select_dtypes(['category']).columns\n",
    "df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "x_columns = df.columns.drop('target')\n",
    "x = df[x_columns].values\n",
    "y = df['target']\n",
    "\n",
    "print(\"Ready to generate train and test datasets\")\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=seed)\n",
    "print(\"x_train, y_train, x_test, y_test\" + str(x_train.shape) + \"\" +str(y_train.shape) + \"\" +str(x_test.shape) + \"\" +str(y_test.shape))\n",
    "'''\n",
    "\n",
    "#train\n",
    "#print(dftrain.loc[dftrain['target'] == 'legitimate'])\n",
    "class_names = dftrain.target.unique()\n",
    "dftrain=dftrain.astype('category')\n",
    "cat_columns = dftrain.select_dtypes(['category']).columns\n",
    "dftrain[cat_columns] = dftrain[cat_columns].apply(lambda x: x.cat.codes)\n",
    "#print(dftrain.loc[125, 'target'])\n",
    "x_columns = dftrain.columns.drop('target')\n",
    "x_train = dftrain[x_columns].values\n",
    "y_train = dftrain['target']\n",
    "\n",
    "#test\n",
    "class_names = dftest.target.unique()\n",
    "dftest=dftest.astype('category')\n",
    "cat_columns = dftest.select_dtypes(['category']).columns\n",
    "dftest[cat_columns] = dftest[cat_columns].apply(lambda x: x.cat.codes)\n",
    "x_columns = dftest.columns.drop('target')\n",
    "x_test = dftest[x_columns].values\n",
    "y_test = dftest['target']\n",
    "\n",
    "\n",
    "print(\"Ready to generate train and test datasets\")\n",
    "\n",
    "#Neural network\n",
    "print(\"Starting Random forest\")\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(30, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(20, kernel_initializer='normal'))\n",
    "model.add(Dense(6,activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "history = model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=200,batch_size=1000) \n",
    "end = time.time()\n",
    "diff=end-start\n",
    "print(\"Training time: \" + str(diff))\n",
    "starttest = time.time()\n",
    "y_pred_nn = model.predict(x_test)\n",
    "y_pred_nn = np.argmax(y_pred_nn,axis=1)\n",
    "endtest =time.time()\n",
    "difftest = endtest-starttest\n",
    "print(\"Test time: \" + str(difftest))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "#RandomForest\n",
    "print(\"Starting Random forest\")\n",
    "classifier = RandomForestClassifier(verbose=2,random_state=seed)\n",
    "classifier.fit(x_train, y_train)\n",
    "end = time.time()\n",
    "diff=end-start\n",
    "print(\"Training time: \" + str(diff))\n",
    "starttest = time.time()\n",
    "y_pred_random = classifier.predict(x_test)\n",
    "endtest =time.time()\n",
    "difftest = endtest-starttest\n",
    "print(\"Test time: \" + str(difftest))\n",
    "\n",
    "#Create Naive Bayes Classifier\n",
    "print(\"Starting Naive Bayes\")\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n",
    "end = time.time()\n",
    "diff=end-start\n",
    "print(\"Training time: \" + str(diff))\n",
    "starttest = time.time()\n",
    "y_pred_nb = gnb.predict(x_test)\n",
    "endtest =time.time()\n",
    "difftest = endtest-starttest\n",
    "print(\"Test time: \" + str(difftest))\n",
    "\n",
    "#Decision tree\n",
    "print(\"Starting Decision tree\")\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(x_train,y_train)\n",
    "end = time.time()\n",
    "diff=end-start\n",
    "print(\"Training time: \" + str(diff))\n",
    "starttest = time.time()\n",
    "y_pred_dt = clf.predict(x_test)\n",
    "y_pred_dt_roc = clf.predict_proba(x_test)\n",
    "endtest =time.time()\n",
    "difftest = endtest-starttest\n",
    "print(\"Test time: \" + str(difftest))\n",
    "\n",
    "#Multi layer perceptron\n",
    "print(\"Starting Multi layer perceptron\")\n",
    "model = MLPClassifier( max_iter=130, batch_size=1000, alpha=1e-4, activation = 'relu',solver='adam', verbose=10, tol=1e-4, random_state=seed)\n",
    "model.fit(x_train, y_train)\n",
    "end = time.time()\n",
    "diff=end-start\n",
    "print(\"Training time: \" + str(diff))\n",
    "starttest = time.time()\n",
    "y_pred_mlp = model.predict(x_test)\n",
    "endtest =time.time()\n",
    "difftest = endtest-starttest\n",
    "print(\"Test time: \" + str(difftest))\n",
    "\n",
    "#Gradient boost\n",
    "print(\"Starting Gradient boost\")\n",
    "model = GradientBoostingClassifier(n_estimators=20, random_state=seed,verbose=2)\n",
    "model.fit(x_train, y_train)\n",
    "end = time.time()\n",
    "diff=end-start\n",
    "print(\"Training time: \" + str(diff))\n",
    "starttest = time.time()\n",
    "y_pred_gradient = model.predict(x_test)\n",
    "endtest =time.time()\n",
    "difftest = endtest-starttest\n",
    "print(\"Test time: \" + str(difftest))\n",
    "\n",
    "\n",
    "print(\"Decision Tree, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_dt)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_dt,average='weighted')))\n",
    "matrixdt = confusion_matrix(y_test,y_pred_dt)\n",
    "print(matrixdt)\n",
    "\n",
    "\n",
    "print(\"Naive Bayes, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_nb)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_nb,average='weighted')))\n",
    "matrixnv = confusion_matrix(y_test,y_pred_nb)\n",
    "print(matrixnv)\n",
    "\n",
    "\n",
    "print(\"Neural network, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_nn)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_nn,average='weighted')))\n",
    "matrixnn = confusion_matrix(y_test,y_pred_nn)\n",
    "print(matrixnn)\n",
    "\n",
    "print(\"MultiLayerPerceptron, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_mlp)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_mlp,average='weighted')))\n",
    "matrixml = confusion_matrix(y_test,y_pred_mlp)\n",
    "print(matrixml)\n",
    "print(\"Random Forest, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_random)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_random,average='weighted')))\n",
    "matrixrf = confusion_matrix(y_test,y_pred_random)\n",
    "print(matrixrf)\n",
    "print(\"GradienBoost, accuracy: \" + str(metrics.accuracy_score(y_test, y_pred_gradient)) + \" F1 score:\" + str(metrics.f1_score(y_test, y_pred_gradient,average='weighted')))\n",
    "matrixgb = confusion_matrix(y_test,y_pred_gradient)\n",
    "print(matrixgb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
